{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "Autograd(自动微分): Automatic Differentiation\n",
        "===================================\n",
        "\n",
        "PyTorch中所有神經網絡的核心即是 ``autograd`` 軟件包。\n",
        "让我们先简要地介绍一下，然后再来训练第一个神经网络。\n",
        "\n",
        "\n",
        "``autograd`` 软件包为Tensor上的所有操作提供自动微分。 \n",
        "这是一个按运行定义(define-by-run)的框架，这意味着您的反向传播(backprop)是由代码的运行方式定义的，并且每次迭代都可以不同。\n",
        "\n",
        "让我们通过一些例子来更简单地了解这一点。\n",
        "\n",
        "Tensor(张量)\n",
        "--------\n",
        "\n",
        "``torch.Tensor`` 是軟件包的中心类別。 \n",
        "如果将其属性``.requires_grad``设置为``True``，它将开始追踪对其的所有操作。\n",
        "完成计算后，可以调用 ``.backward()``并自动计算所有梯度。 该张量的梯度将累积到``.grad``属性中。\n",
        "\n",
        "若要使张量停止追踪历史记录，您可以调用``.detach()``将其从计算历史记录中分离出来，并防止将来的计算被追踪。\n",
        "\n",
        "若要防止追踪历史记录（和占用内存），您还可以将代码块包装在``with torch.no_grad():``中。    \n",
        "这在评估模型时特别有用，因为该模型可能具有带有`requires_grad=True`的可训练参数，但是我们不需要梯度。\n",
        "\n",
        "\n",
        "还有另外一个类别对实现autograd非常重要-称为``Function``。\n",
        "\n",
        "``Tensor``和``Function``相互连接并建立一个非循环图形(acyclic graph)，它编码完整的计算历史记录。     \n",
        "每个张量都有一个``.grad_fn``属性，该属性引用创建了``Tensor``的``Function``（用户创建的Tensors除外-它们的``grad_fn``為``None``）。\n",
        "\n",
        "如果要计算导数，可以在``Tensor``上调用``.backward()``。     \n",
        "如果``Tensor``是标量（即，它保存一个元素数据），则无需为``.backward()``指定任何参数，但是，如果它具有更多元素，则需要指定``gradient``实际参数(argument)是匹配形状的张量。\n",
        "\n",
        "譯註：  \n",
        "1. `requires_grad`默認為False，即「不需要求導」    \n",
        "2. 在张量间的计算过程中，如果在所有输入中，有任一输入需要求导，那么输出則会需要求导   \n",
        "3. 在PyTorch中，``Tensor``与``tensor``不一样，须注意\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "创建一个张量并设置``require_grad = True``来追踪它的计算\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": false,
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "tensor([[1., 1.],\n        [1., 1.]], requires_grad=True)\n"
        }
      ],
      "source": [
        "x = torch.ones(2, 2, requires_grad=True)\nprint(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "进行一次张量运算：\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "collapsed": false,
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "tensor([[3., 3.],\n        [3., 3.]], grad_fn=<AddBackward0>)\n"
        }
      ],
      "source": [
        "y = x + 2\nprint(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "``y``是由于操作而创建的，因此具有``grad_fn``。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "collapsed": false,
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "<AddBackward0 object at 0x000001DA3B98A070>\n"
        }
      ],
      "source": [
        "print(y.grad_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "对``y``进行更多操作\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "collapsed": false,
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "tensor([[27., 27.],\n        [27., 27.]], grad_fn=<MulBackward0>) tensor(27., grad_fn=<MeanBackward0>)\n"
        }
      ],
      "source": [
        "z = y * y * 3\nout = z.mean()\n\nprint(z, out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "``.requires_grad_( ... )``就地(in-place)更改现有Tensor的``requires_grad``标志。 如果未给出输入标志，则默认为False。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "collapsed": false,
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "False\nTrue\n<SumBackward0 object at 0x000001DA3B984DC0>\n"
        }
      ],
      "source": [
        "a = torch.randn(2, 2)\na = ((a * 3) / (a - 1))\nprint(a.requires_grad)\na.requires_grad_(True)\nprint(a.requires_grad)\nb = (a * a).sum()\nprint(b.grad_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Gradients(梯度)\n",
        "---------\n",
        "让我们现在进行反向传播。    \n",
        "因为``out``包含单个标量，所以``out.backward()``等同于``out.backward(torch.tensor(1))``。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "out.backward()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "打印梯度) d(out)/dx\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "collapsed": false,
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "tensor([[4.5000, 4.5000],\n        [4.5000, 4.5000]])\n"
        }
      ],
      "source": [
        "print(x.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "您应该会得到一个``4.5``的矩阵。     \n",
        "让我们称呼``out``*张量*为“$o$”.     \n",
        "$o = \\frac{1}{4}\\sum_i z_i$,\n",
        "$z_i = 3(x_i+2)^2$ 且 $z_i\\bigr\\rvert_{x_i=1} = 27$.\n",
        "因此,\n",
        "$\\frac{\\partial o}{\\partial x_i} = \\frac{3}{2}(x_i+2)$, 所以\n",
        "$\\frac{\\partial o}{\\partial x_i}\\bigr\\rvert_{x_i=1} = \\frac{9}{2} = 4.5$.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "您可以使用autograd做许多疯狂的事情！\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "collapsed": false,
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "tensor([ 833.0659, -209.5000, -988.5157], grad_fn=<MulBackward0>)\n"
        }
      ],
      "source": [
        "x = torch.randn(3, requires_grad=True)\n\ny = x * 2\nwhile y.data.norm() < 1000:\n    y = y * 2\n\nprint(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "collapsed": false,
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "tensor([2.0480e+02, 2.0480e+03, 2.0480e-01])\n"
        }
      ],
      "source": [
        "gradients = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)\ny.backward(gradients)\n\nprint(x.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can also stop autograd from tracking history on Tensors with ``.requires_grad=True`` by wrapping the code block in ``with torch.no_grad()``:\n",
        "您还可以通过将代码块包装在``with torch.no_grad()``中来阻止autograd追踪带有``.requires_grad=True``的张量的历史纪录。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "collapsed": false,
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "True\nTrue\nFalse\n"
        }
      ],
      "source": [
        "print(x.requires_grad)\nprint((x ** 2).requires_grad)\n\nwith torch.no_grad():\n\tprint((x ** 2).requires_grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**稍后阅读:**\n",
        "\n",
        "``autograd``和``Function``的文档位于\n",
        "https://pytorch.org/docs/autograd\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3-candidate"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}